{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#table to be used as loc dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from urllib.request import urlopen, Request\n",
    "from urllib.error import URLError, HTTPError\n",
    "import pandas as pd\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurações de logging e constantes\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "URL = \"https://servicodados.ibge.gov.br/api/v1/localidades/municipios\"\n",
    "CITIES = \"cities\"\n",
    "STATES = \"states\"\n",
    "HEADERS = {CITIES: (\"code\", \"name\", \"state\"), STATES: (\"code\", \"abbr\", \"name\")}\n",
    "FORMATS = (\"csv\", \"parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_ibge_data(state_filter=None):\n",
    "    \"\"\"\n",
    "    Função para extrair dados do IBGE e processar cidades e estados.\n",
    "    Utiliza um gerador para evitar sobrecarregar a memória com grandes volumes de dados.\n",
    "    Lida com possíveis respostas comprimidas.\n",
    "\n",
    "    Parâmetros:\n",
    "    state_filter: String opcional para filtrar os dados por estado (UF).\n",
    "                  Se None, retorna todos os estados e municípios.\n",
    "    \"\"\"\n",
    "    states_yielded = set()\n",
    "    \n",
    "    logging.info(f\"Fetching data from {URL}…\")\n",
    "\n",
    "    try:\n",
    "        request = Request(URL)\n",
    "        request.add_header('Accept-encoding', 'gzip') \n",
    "        with urlopen(request) as response:\n",
    "            if response.info().get('Content-Encoding') == 'gzip':\n",
    "                logging.info(\"Response is compressed with gzip.\")\n",
    "                with gzip.GzipFile(fileobj=response) as decompressed:\n",
    "                    data = json.loads(decompressed.read().decode('utf-8'))\n",
    "            else:\n",
    "                data = json.loads(response.read().decode('utf-8'))\n",
    "    except HTTPError as e:\n",
    "        logging.error(f\"HTTP error occurred: {e.code} - {e.reason}\")\n",
    "        return\n",
    "    except URLError as e:\n",
    "        logging.error(f\"Failed to reach the server: {e.reason}\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An error occurred: {e}\")\n",
    "        return\n",
    "\n",
    "    for obj in data:\n",
    "        city = {\n",
    "            \"code\": str(obj[\"id\"])[:6],\n",
    "            \"name\": obj[\"nome\"],\n",
    "            \"state\": obj[\"microrregiao\"][\"mesorregiao\"][\"UF\"][\"sigla\"],\n",
    "        }\n",
    "\n",
    "        # Aplica o filtro de estado se estiver definido\n",
    "        if state_filter and city[\"state\"] != state_filter:\n",
    "            continue\n",
    "\n",
    "        yield (CITIES, city)\n",
    "\n",
    "        if city[\"state\"] in states_yielded:\n",
    "            continue\n",
    "\n",
    "        state = {\n",
    "            \"abbr\": city[\"state\"],\n",
    "            \"code\": obj[\"microrregiao\"][\"mesorregiao\"][\"UF\"][\"id\"],\n",
    "            \"name\": obj[\"microrregiao\"][\"mesorregiao\"][\"UF\"][\"nome\"],\n",
    "        }\n",
    "        yield (STATES, state)\n",
    "        states_yielded.add(state[\"abbr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataWriter:\n",
    "    \"\"\"\n",
    "    Classe responsável por salvar os dados em arquivos CSV e Parquet.\n",
    "    Implementa boas práticas de modulação, com separação clara de responsabilidades.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_generator, output_dir):\n",
    "        self.data_generator = data_generator\n",
    "        self.data = {CITIES: [], STATES: []}\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.output_dir.mkdir(parents=True, exist_ok=True)  # Cria diretórios se não existirem\n",
    "        self.paths = self._generate_paths()\n",
    "\n",
    "    def _generate_paths(self):\n",
    "        \"\"\"\n",
    "        Gera os caminhos para salvar os arquivos CSV e Parquet.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            f\"{name}.{ext}\": self.output_dir / f\"{name}.{ext}\"\n",
    "            for name in HEADERS for ext in FORMATS\n",
    "        }\n",
    "\n",
    "    def _sort_data(self):\n",
    "        \"\"\"\n",
    "        Ordena os dados por nome para facilitar a leitura.\n",
    "        \"\"\"\n",
    "        logging.info(\"Sorting data…\")\n",
    "        for name in HEADERS:\n",
    "            self.data[name] = sorted(self.data[name], key=lambda row: row[\"name\"])\n",
    "\n",
    "    def _write_csv(self):\n",
    "        \"\"\"\n",
    "        Salva os dados no formato CSV.\n",
    "        \"\"\"\n",
    "        for name, headers in HEADERS.items():\n",
    "            csv_path = self.paths[f\"{name}.csv\"]\n",
    "            with csv_path.open(\"w\", encoding=\"utf-8\", newline='') as file:\n",
    "                writer = csv.DictWriter(file, fieldnames=headers)\n",
    "                writer.writeheader()\n",
    "                for line in self.data[name]:\n",
    "                    writer.writerow(line)\n",
    "            logging.info(f\"CSV file saved: {csv_path}\")\n",
    "\n",
    "    def _write_parquet(self):\n",
    "        \"\"\"\n",
    "        Salva os dados no formato Parquet.\n",
    "        \"\"\"\n",
    "        for name in HEADERS:\n",
    "            parquet_path = self.paths[f\"{name}.parquet\"]\n",
    "            df = pd.DataFrame(self.data[name])\n",
    "            df.to_parquet(parquet_path, index=False)\n",
    "            logging.info(f\"Parquet file saved: {parquet_path}\")\n",
    "\n",
    "    def _cleanup_existing_files(self):\n",
    "        \"\"\"\n",
    "        Remove arquivos antigos antes de salvar novos arquivos.\n",
    "        \"\"\"\n",
    "        for path in self.paths.values():\n",
    "            if path.exists():\n",
    "                path.unlink()\n",
    "                logging.info(f\"Deleted existing file: {path}\")\n",
    "\n",
    "    def save_data(self):\n",
    "        \"\"\"\n",
    "        Método principal para iniciar o processo de salvar dados.\n",
    "        \"\"\"\n",
    "        for name, row in self.data_generator:\n",
    "            self.data[name].append(row)\n",
    "\n",
    "        self._sort_data()\n",
    "        self._cleanup_existing_files()\n",
    "        self._write_csv()\n",
    "        # self._write_parquet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(output_dir, state_filter=None):\n",
    "    \"\"\"\n",
    "    Função principal para orquestrar a extração e salvamento dos dados.\n",
    "    O parâmetro output_dir define o diretório onde os arquivos CSV e Parquet serão salvos.\n",
    "    O parâmetro state_filter define se deve filtrar por um estado específico.\n",
    "    \"\"\"\n",
    "    data_generator = fetch_ibge_data(state_filter)\n",
    "    if data_generator:\n",
    "        writer = DataWriter(data_generator, output_dir)\n",
    "        writer.save_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-15 17:57:44,039 - INFO - Fetching data from https://servicodados.ibge.gov.br/api/v1/localidades/municipios…\n",
      "2024-09-15 17:57:44,253 - INFO - Response is compressed with gzip.\n",
      "2024-09-15 17:57:44,468 - INFO - Sorting data…\n",
      "2024-09-15 17:57:44,469 - INFO - Deleted existing file: ..\\..\\dataDiscovery\\ibge\\sample\\cities.csv\n",
      "2024-09-15 17:57:44,470 - INFO - Deleted existing file: ..\\..\\dataDiscovery\\ibge\\sample\\states.csv\n",
      "2024-09-15 17:57:44,471 - INFO - CSV file saved: ..\\..\\dataDiscovery\\ibge\\sample\\cities.csv\n",
      "2024-09-15 17:57:44,472 - INFO - CSV file saved: ..\\..\\dataDiscovery\\ibge\\sample\\states.csv\n",
      "2024-09-15 17:57:44,473 - INFO - Process completed successfully!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    output_dir = \"../../dataDiscovery/ibge/sample/\" \n",
    "    state_filter = \"AC\"  \n",
    "    main(output_dir, state_filter)\n",
    "    logging.info(\"Process completed successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
